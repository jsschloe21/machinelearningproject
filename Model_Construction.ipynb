{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93bdca3e-3c00-4477-a616-c064d4c04f32",
   "metadata": {},
   "source": [
    "**Names**: Josh Schloe\n",
    "\n",
    "<br>\n",
    "\n",
    "**Goal**: Your goal is ultimately to create the best model possible. This will need to be submitted\n",
    "separately so that it can be evaluated against the hidden test set. Your model script should accept a\n",
    "Pandas DataFrame as input, with columns labeled the same as in the training data set, so that all\n",
    "necessary preprocessing steps can be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39085a95-0746-468c-b6ae-4a9b02143a4b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b17bac-6eaf-4829-a47e-c88d1966c9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "# Basics and Plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "from itertools import chain, combinations\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Specifics\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import metrics \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Alternative models\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ace323-76e6-4f9b-a8c5-602bbf90a9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90.0</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>204000.00</td>\n",
       "      <td>2.1</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75.0</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>127000.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>454000.00</td>\n",
       "      <td>1.1</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.5</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>388000.00</td>\n",
       "      <td>9.4</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a  b     c  d   e  f          h    i    j  k  l   m  y\n",
       "0  75.0  0   582  0  20  1  265000.00  1.9  130  1  0   4  1\n",
       "1  55.0  0  7861  0  38  0  263358.03  1.1  136  1  0   6  1\n",
       "2  65.0  0   146  0  20  0  162000.00  1.3  129  1  1   7  1\n",
       "3  50.0  1   111  0  20  0  210000.00  1.9  137  1  0   7  1\n",
       "4  65.0  1   160  1  20  0  327000.00  2.7  116  0  0   8  1\n",
       "5  90.0  1    47  0  40  1  204000.00  2.1  132  1  1   8  1\n",
       "6  75.0  1   246  0  15  0  127000.00  1.2  137  1  0  10  1\n",
       "7  60.0  1   315  1  60  0  454000.00  1.1  131  1  1  10  1\n",
       "8  65.0  0   157  0  65  0  263358.03  1.5  138  0  0  10  1\n",
       "9  80.0  1   123  0  35  1  388000.00  9.4  133  1  1  10  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "data = pd.read_csv('TRAIN.csv')  \n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5d610-63c3-4bd8-9db6-4e8ea4f1544f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c46c593-0cc5-4201-bc9b-e7cff5377a51",
   "metadata": {},
   "source": [
    "### Normalize The Data\n",
    "\n",
    "Normalizing the data is important for a variety of reasons. For starters, it creates a consistent scale between features. Since a fair amount of machine learning algorithms rely on the scale of features, and features with larger scales might dominate those with smaller scales, standardizing the data makes sure that all features have the same scale. This prevents one feature from excessively impacting the model. It also helps with the interpretability of the data/coefficients. In many models, the coefficients represent the change in the response variable for a one-unit change in a given feature. When features are not standardized, comparing the strength of coefficients is much harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1fc33ed-3a9d-414e-94c3-afd7a579ba37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db143851-03d3-46c4-8b82-39cc2c6f6427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfc = data[[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d43beb1-393d-4175-ba0c-73be96218a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.07082696, -0.95118973, -0.01461188, ...,  0.73379939,\n",
       "        -0.7097601 , -1.66573626],\n",
       "       [-0.57934985, -0.95118973,  6.62251069, ...,  0.73379939,\n",
       "        -0.7097601 , -1.62437206],\n",
       "       [ 0.24573856, -0.95118973, -0.41216449, ...,  0.73379939,\n",
       "         1.40892676, -1.60368996],\n",
       "       ...,\n",
       "       [ 0.24573856, -0.95118973, -0.39301632, ..., -1.36277029,\n",
       "        -0.7097601 ,  2.09840577],\n",
       "       [-0.99189406,  1.05131497, -0.01461188, ..., -1.36277029,\n",
       "        -0.7097601 ,  2.09840577],\n",
       "       [-0.16680565, -0.95118973,  0.55892158, ...,  0.73379939,\n",
       "         1.40892676,  2.09840577]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17441264-4349-4db1-b441-7428f49ce771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a        62.02167\n",
       "b         0.47500\n",
       "c       598.02500\n",
       "d         0.41500\n",
       "e        37.95500\n",
       "f         0.39500\n",
       "h    261477.22270\n",
       "i         1.44315\n",
       "j       136.52500\n",
       "k         0.65000\n",
       "l         0.33500\n",
       "m        84.54000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847f7ed8-38a9-493a-be1e-671eac5975b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.068147</td>\n",
       "      <td>-0.948809</td>\n",
       "      <td>-0.014575</td>\n",
       "      <td>-0.840152</td>\n",
       "      <td>-1.419543</td>\n",
       "      <td>1.234499</td>\n",
       "      <td>0.038143</td>\n",
       "      <td>0.435935</td>\n",
       "      <td>-1.418978</td>\n",
       "      <td>0.731963</td>\n",
       "      <td>-0.707983</td>\n",
       "      <td>-1.661567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.577900</td>\n",
       "      <td>-0.948809</td>\n",
       "      <td>6.605934</td>\n",
       "      <td>-0.840152</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>-0.805995</td>\n",
       "      <td>0.020365</td>\n",
       "      <td>-0.327440</td>\n",
       "      <td>-0.114171</td>\n",
       "      <td>0.731963</td>\n",
       "      <td>-0.707983</td>\n",
       "      <td>-1.620306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.245123</td>\n",
       "      <td>-0.948809</td>\n",
       "      <td>-0.411133</td>\n",
       "      <td>-0.840152</td>\n",
       "      <td>-1.419543</td>\n",
       "      <td>-0.805995</td>\n",
       "      <td>-1.077095</td>\n",
       "      <td>-0.136596</td>\n",
       "      <td>-1.636446</td>\n",
       "      <td>0.731963</td>\n",
       "      <td>1.405400</td>\n",
       "      <td>-1.599676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.989411</td>\n",
       "      <td>1.048683</td>\n",
       "      <td>-0.442967</td>\n",
       "      <td>-0.840152</td>\n",
       "      <td>-1.419543</td>\n",
       "      <td>-0.805995</td>\n",
       "      <td>-0.557372</td>\n",
       "      <td>0.435935</td>\n",
       "      <td>0.103297</td>\n",
       "      <td>0.731963</td>\n",
       "      <td>-0.707983</td>\n",
       "      <td>-1.599676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.245123</td>\n",
       "      <td>1.048683</td>\n",
       "      <td>-0.398399</td>\n",
       "      <td>1.184310</td>\n",
       "      <td>-1.419543</td>\n",
       "      <td>-0.805995</td>\n",
       "      <td>0.709451</td>\n",
       "      <td>1.199309</td>\n",
       "      <td>-4.463528</td>\n",
       "      <td>-1.359359</td>\n",
       "      <td>-0.707983</td>\n",
       "      <td>-1.579045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c         d         e         f         h  \\\n",
       "0  1.068147 -0.948809 -0.014575 -0.840152 -1.419543  1.234499  0.038143   \n",
       "1 -0.577900 -0.948809  6.605934 -0.840152  0.003558 -0.805995  0.020365   \n",
       "2  0.245123 -0.948809 -0.411133 -0.840152 -1.419543 -0.805995 -1.077095   \n",
       "3 -0.989411  1.048683 -0.442967 -0.840152 -1.419543 -0.805995 -0.557372   \n",
       "4  0.245123  1.048683 -0.398399  1.184310 -1.419543 -0.805995  0.709451   \n",
       "\n",
       "          i         j         k         l         m  \n",
       "0  0.435935 -1.418978  0.731963 -0.707983 -1.661567  \n",
       "1 -0.327440 -0.114171  0.731963 -0.707983 -1.620306  \n",
       "2 -0.136596 -1.636446  0.731963  1.405400 -1.599676  \n",
       "3  0.435935  0.103297  0.731963 -0.707983 -1.599676  \n",
       "4  1.199309 -4.463528 -1.359359 -0.707983 -1.579045  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc = (dfc - dfc.mean())/dfc.std()\n",
    "dfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98b28093-a360-4fb3-81a4-a913454db4f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfc['y'] = data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dceaea7-eb18-4913-a036-85557f45a117",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.068147</td>\n",
       "      <td>-0.948809</td>\n",
       "      <td>-0.014575</td>\n",
       "      <td>-0.840152</td>\n",
       "      <td>-1.419543</td>\n",
       "      <td>1.234499</td>\n",
       "      <td>0.038143</td>\n",
       "      <td>0.435935</td>\n",
       "      <td>-1.418978</td>\n",
       "      <td>0.731963</td>\n",
       "      <td>-0.707983</td>\n",
       "      <td>-1.661567</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.577900</td>\n",
       "      <td>-0.948809</td>\n",
       "      <td>6.605934</td>\n",
       "      <td>-0.840152</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>-0.805995</td>\n",
       "      <td>0.020365</td>\n",
       "      <td>-0.327440</td>\n",
       "      <td>-0.114171</td>\n",
       "      <td>0.731963</td>\n",
       "      <td>-0.707983</td>\n",
       "      <td>-1.620306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.245123</td>\n",
       "      <td>-0.948809</td>\n",
       "      <td>-0.411133</td>\n",
       "      <td>-0.840152</td>\n",
       "      <td>-1.419543</td>\n",
       "      <td>-0.805995</td>\n",
       "      <td>-1.077095</td>\n",
       "      <td>-0.136596</td>\n",
       "      <td>-1.636446</td>\n",
       "      <td>0.731963</td>\n",
       "      <td>1.405400</td>\n",
       "      <td>-1.599676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.989411</td>\n",
       "      <td>1.048683</td>\n",
       "      <td>-0.442967</td>\n",
       "      <td>-0.840152</td>\n",
       "      <td>-1.419543</td>\n",
       "      <td>-0.805995</td>\n",
       "      <td>-0.557372</td>\n",
       "      <td>0.435935</td>\n",
       "      <td>0.103297</td>\n",
       "      <td>0.731963</td>\n",
       "      <td>-0.707983</td>\n",
       "      <td>-1.599676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.245123</td>\n",
       "      <td>1.048683</td>\n",
       "      <td>-0.398399</td>\n",
       "      <td>1.184310</td>\n",
       "      <td>-1.419543</td>\n",
       "      <td>-0.805995</td>\n",
       "      <td>0.709451</td>\n",
       "      <td>1.199309</td>\n",
       "      <td>-4.463528</td>\n",
       "      <td>-1.359359</td>\n",
       "      <td>-0.707983</td>\n",
       "      <td>-1.579045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c         d         e         f         h  \\\n",
       "0  1.068147 -0.948809 -0.014575 -0.840152 -1.419543  1.234499  0.038143   \n",
       "1 -0.577900 -0.948809  6.605934 -0.840152  0.003558 -0.805995  0.020365   \n",
       "2  0.245123 -0.948809 -0.411133 -0.840152 -1.419543 -0.805995 -1.077095   \n",
       "3 -0.989411  1.048683 -0.442967 -0.840152 -1.419543 -0.805995 -0.557372   \n",
       "4  0.245123  1.048683 -0.398399  1.184310 -1.419543 -0.805995  0.709451   \n",
       "\n",
       "          i         j         k         l         m  y  \n",
       "0  0.435935 -1.418978  0.731963 -0.707983 -1.661567  1  \n",
       "1 -0.327440 -0.114171  0.731963 -0.707983 -1.620306  1  \n",
       "2 -0.136596 -1.636446  0.731963  1.405400 -1.599676  1  \n",
       "3  0.435935  0.103297  0.731963 -0.707983 -1.599676  1  \n",
       "4  1.199309 -4.463528 -1.359359 -0.707983 -1.579045  1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62000096-7cd3-46ab-a809-cb0056a7556c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   9.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 03 Feb 2024</td> <th>  Prob (F-statistic):</th> <td>4.45e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:11:30</td>     <th>  Log-Likelihood:    </th> <td> -96.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   219.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   187</td>      <th>  BIC:               </th> <td>   262.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.4450</td> <td>    0.029</td> <td>   15.493</td> <td> 0.000</td> <td>    0.388</td> <td>    0.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>a</th>         <td>    0.1084</td> <td>    0.031</td> <td>    3.510</td> <td> 0.001</td> <td>    0.047</td> <td>    0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>b</th>         <td>   -0.0124</td> <td>    0.030</td> <td>   -0.406</td> <td> 0.685</td> <td>   -0.072</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>c</th>         <td>    0.0352</td> <td>    0.030</td> <td>    1.168</td> <td> 0.244</td> <td>   -0.024</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>d</th>         <td>    0.0159</td> <td>    0.029</td> <td>    0.540</td> <td> 0.590</td> <td>   -0.042</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>e</th>         <td>   -0.1477</td> <td>    0.030</td> <td>   -4.949</td> <td> 0.000</td> <td>   -0.207</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>f</th>         <td>   -0.0137</td> <td>    0.030</td> <td>   -0.462</td> <td> 0.645</td> <td>   -0.072</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>h</th>         <td>   -0.0136</td> <td>    0.029</td> <td>   -0.464</td> <td> 0.643</td> <td>   -0.071</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>i</th>         <td>    0.0723</td> <td>    0.031</td> <td>    2.364</td> <td> 0.019</td> <td>    0.012</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>j</th>         <td>   -0.0289</td> <td>    0.030</td> <td>   -0.962</td> <td> 0.337</td> <td>   -0.088</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>k</th>         <td>   -0.0493</td> <td>    0.033</td> <td>   -1.476</td> <td> 0.142</td> <td>   -0.115</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>l</th>         <td>    0.0024</td> <td>    0.033</td> <td>    0.075</td> <td> 0.940</td> <td>   -0.062</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>m</th>         <td>   -0.1843</td> <td>    0.030</td> <td>   -6.101</td> <td> 0.000</td> <td>   -0.244</td> <td>   -0.125</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 7.310</td> <th>  Durbin-Watson:     </th> <td>   1.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.026</td> <th>  Jarque-Bera (JB):  </th> <td>   5.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.297</td> <th>  Prob(JB):          </th> <td>  0.0615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.438</td> <th>  Cond. No.          </th> <td>    1.82</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.375\n",
       "Model:                            OLS   Adj. R-squared:                  0.335\n",
       "Method:                 Least Squares   F-statistic:                     9.363\n",
       "Date:                Sat, 03 Feb 2024   Prob (F-statistic):           4.45e-14\n",
       "Time:                        21:11:30   Log-Likelihood:                -96.889\n",
       "No. Observations:                 200   AIC:                             219.8\n",
       "Df Residuals:                     187   BIC:                             262.7\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.4450      0.029     15.493      0.000       0.388       0.502\n",
       "a              0.1084      0.031      3.510      0.001       0.047       0.169\n",
       "b             -0.0124      0.030     -0.406      0.685      -0.072       0.048\n",
       "c              0.0352      0.030      1.168      0.244      -0.024       0.095\n",
       "d              0.0159      0.029      0.540      0.590      -0.042       0.074\n",
       "e             -0.1477      0.030     -4.949      0.000      -0.207      -0.089\n",
       "f             -0.0137      0.030     -0.462      0.645      -0.072       0.045\n",
       "h             -0.0136      0.029     -0.464      0.643      -0.071       0.044\n",
       "i              0.0723      0.031      2.364      0.019       0.012       0.133\n",
       "j             -0.0289      0.030     -0.962      0.337      -0.088       0.030\n",
       "k             -0.0493      0.033     -1.476      0.142      -0.115       0.017\n",
       "l              0.0024      0.033      0.075      0.940      -0.062       0.067\n",
       "m             -0.1843      0.030     -6.101      0.000      -0.244      -0.125\n",
       "==============================================================================\n",
       "Omnibus:                        7.310   Durbin-Watson:                   1.512\n",
       "Prob(Omnibus):                  0.026   Jarque-Bera (JB):                5.577\n",
       "Skew:                           0.297   Prob(JB):                       0.0615\n",
       "Kurtosis:                       2.438   Cond. No.                         1.82\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = smf.ols(formula = \"y ~ 1 + a + b + c + d + e + f + h + i + j + k + l + m\", data = dfc).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2a67dcd-285f-44a3-827a-680919071e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf6f11-8fef-41bd-a8e5-7cb4bb4f11a0",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "A decision tree is a well known machine learning model used for both classification and regression problems. It gets its name from its tree-like structure where each internal node depicts a decision based on the value of a specific feature. Each leaf node thus represents the predicted outcome. Some benefits of using decision trees is that they are easy to understand, interpret, and visualize, making them easy to utilize with a variety of data. Decision trees also have limitations. They tend to show sensitivity to noisy data, instability with small variations in data, and of course, there is the potential for overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbe536-353f-4800-93dc-03179f05259b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Standard Decision Tree\n",
    "\n",
    "To begin with decision trees, we started by constructing a simple standard decision tree. This is the most basic version of decision tree with the goal of creating a simple a tree structure that can efficiently and effectively make decisions about the response variable.\n",
    "\n",
    "We also use K-Fold cross validation to help assess the performance of the standard decision tree. The primary goal of K-fold cross-validation is to help grasp a more reliable estimate of how the decision tree is performing by splitting the dataset into K subgroupings and using each group for both training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52fa0cc5-abe6-4104-8e4c-4a10d3b171c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Feature Selection\n",
    "###########################\n",
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afe206ed-9eba-4c15-a3d5-71852d897dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.76 [0.775 0.725 0.725 0.725 0.85 ]\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "## K-fold cross-validation for Decision Tree\n",
    "############################\n",
    "folds = 5  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "# create LDA model\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_results = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Mean Accuracy:\", cv_results.mean(), cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8404c33-6f59-403e-bedc-a082f1ca68e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95098b9e-dc94-4031-96f3-b7e79448882d",
   "metadata": {},
   "source": [
    "### Random Forest ---> Very similar to Bagged\n",
    "\n",
    "A Random Forest is a method very similar to the previously used Bagged. The main difference between the two can be found in the additional randomness that is introduced. This is done by limiting which features are allowed to be considered at each individual split for each tree in the bagged ensemble. This helps to stop the correlation of trees. This means that although a preedictor may be strong, not all the splits in the trees will be able to use it.\n",
    "\n",
    "The introduction of randomness to the feature selection in a Random Forest reduces the risk of overfitting, making the model less sensitive to noise in the training data. The randomness also helps the model capture in depth relationships in the data more effectively. This increase in mean accuracy using the Random Forest model can be shown below. \n",
    "\n",
    "To even further better the Random Forest, of course we use cross validation. The hyperparameters able to be adjusted within a Random Forest include the number of estimators, max features, max depth, max leaf nodes, and the criterion by which the data is partitioned. By doing so, we are able to significantly increase the mean accuracy for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aac8087b-648f-4e6b-b8f3-76a738eb677f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Feature Selection\n",
    "###########################\n",
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cfebff-799e-48bd-87d1-932d3fa8e298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "## Parameter Tuning for Random Forest\n",
    "#######################################\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "#pass RandomFoestClassifier() instance to the model and then fit the GridSearchCV using \n",
    "# the training data to find the best parameters.\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators' :np.arange(25,500,50), \n",
    "#    'max_features': ['sqrt', 'log2', None], \n",
    "    'max_depth': [i+1 for i in range(10)], \n",
    "    'max_leaf_nodes': [i+1 for i in range(10)],\n",
    "#    'criterion': ['gini', 'entropy', 'log_loss']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), \n",
    "                           param_grid=param_grid) \n",
    "grid_search.fit(X, y) \n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f65b8-af39-4a5e-b41c-53519413f3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## K-fold cross-validation for Random Forest - Base\n",
    "############################\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "# create RF model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_results = cross_val_score(rf, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Mean Accuracy:\", cv_results.mean(), cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8979d6-48fd-41b1-99c4-b27b57a1b6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## K-fold cross-validation for Random Forest - Best\n",
    "############################\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "# create RF model\n",
    "rf = RandomForestClassifier(criterion='entropy', max_depth=6, max_features='log2',\n",
    "                       max_leaf_nodes=6, n_estimators=50)\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_results = cross_val_score(rf, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Mean Accuracy:\", cv_results.mean(), cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b793d-66de-4a88-9184-409249c1dbad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## K-fold cross-validation for Random Forest\n",
    "# ############################\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "# create LDA model\n",
    "rf = RandomForestClassifier(max_depth=2, max_leaf_nodes=7, n_estimators=25)\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_results = cross_val_score(rf, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Mean Accuracy:\", cv_results.mean(), cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562b699-1200-4390-a1ee-b1e941ba7044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_results = []\n",
    "\n",
    "mod1 = RandomForestClassifier(max_depth=2, max_leaf_nodes=7, n_estimators=25, random_state=1)\n",
    "mod2 = RandomForestClassifier(n_estimators=150, max_depth=3, max_leaf_nodes=6, random_state=1)\n",
    "# mod3 = RandomForestClassifier(criterion='entropy', max_depth=6, max_features='log2',\n",
    "#                               max_leaf_nodes=6, n_estimators=50)\n",
    "\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "models = [mod1, mod2]\n",
    "\n",
    "for model in models:\n",
    "    cv_results_model = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    cv_results.append(cv_results_model)\n",
    "\n",
    "plt.boxplot(cv_results)\n",
    "plt.title(\"Classification Results\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks([1, 2], [\"Random Forest \\n Basic\", \"Random Forest \\n Optimized\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3909a8-ecff-466d-b5db-28a676f561f2",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cc67f-2c5a-4a84-837a-8fa0c38e7773",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bagged Linear Model vs Bagged Trees vs Random Forest\n",
    "\n",
    "By constructing a boxplot of our regression results,  we are able to understand the relationship between the MSEs for the bagged linear model, bagged trees, and the random forest. This shows us that a bagged linear model is by far the worst choice out of these three. Random forest ends up being the best model by a hair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a5b41-6476-4797-8458-b58fa851c1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Feature Selection\n",
    "###########################\n",
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable\n",
    "\n",
    "###########################\n",
    "# Train/test split\n",
    "###########################\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)  # 70% training and 30% test\n",
    "\n",
    "mod1 = ensemble.BaggingRegressor(LinearRegression(), n_estimators=100)\n",
    "mod2 = ensemble.BaggingRegressor(n_estimators=100)\n",
    "mod3 = ensemble.RandomForestRegressor(n_estimators=100, max_features=6)\n",
    "\n",
    "res1 = mod1.fit(X_train, y_train)\n",
    "res2 = mod2.fit(X_train, y_train)\n",
    "res3 = mod3.fit(X_train, y_train)\n",
    "\n",
    "my_mse = [[],[],[]]\n",
    "\n",
    "for i in range(100):\n",
    "    # Test Models on the existing test set\n",
    "    my_mse[0].append(mean_squared_error(res1.predict(X_test), y_test))\n",
    "    my_mse[1].append(mean_squared_error(res2.predict(X_test), y_test))\n",
    "    my_mse[2].append(mean_squared_error(res3.predict(X_test), y_test))\n",
    "    \n",
    "plt.boxplot(my_mse)\n",
    "plt.title(\"Regression Results\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.xticks([1,2,3],[\"Bagged Linear Model\", \"Bagged Trees\", \"Random Forest\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2787b-836c-4d6b-b6d7-4840357610d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Feature Selection\n",
    "###########################\n",
    "\n",
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable\n",
    "cv_results = []\n",
    "\n",
    "mod1 = ensemble.BaggingRegressor(LinearRegression(), n_estimators=100)\n",
    "mod2 = ensemble.BaggingRegressor(n_estimators=100)\n",
    "mod3 = ensemble.RandomForestRegressor(n_estimators=100, max_features=6)\n",
    "\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "models = [mod1, mod2, mod3]\n",
    "\n",
    "for model in models:\n",
    "    cv_results_model = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    cv_results.append(cv_results_model)\n",
    "\n",
    "plt.boxplot(cv_results)\n",
    "plt.title(\"Classification Results\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks([1, 2, 3],[\"Bagged Linear Model\", \"Bagged Trees\", \"Random Forest\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd538555-7c26-49d8-97f9-a2a42c209c8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SVM\n",
    "\n",
    "In this area of our notebook, we tried various models to gauge how each model compared in accuracy. We started with a Support Vector Machine (SVM). The main objective of SVM is to find a hyperplane that is able to best separate the data into different classes. One pro of using SVMs is it works well in high-dimensional spaces meaning it does well with a large number of features. It is also able to utilize kernal functions which help to handle non-linear decision boundaries and capture complex relationships. A con of using an SVM they are very sensitive to noise in the data, and outliers can significantly impact the model's performance. Although this gives us a fairly decent result, it does not exceed the accuracy of the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8611a51-1ea4-472b-af3a-7ef3d1fcc0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaeb9a5-0ab3-4e53-8dfd-b338ef3b412e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# # Feature Selection\n",
    "# ###########################\n",
    "\n",
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869198c-e351-4494-bea1-7fca7946c804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# SVM\n",
    "###########################################\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(-4,4,200), \n",
    "    'tol': np.logspace(-4,4,200), \n",
    "    'degree': [1,2,3,4,5,6,7,8,9],\n",
    " #   'kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "    'shrinking': [True, False]\n",
    "        }\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), \n",
    "                           param_grid=param_grid) \n",
    "grid_search.fit(X, y) \n",
    "print(grid_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46da8bd-17ef-4ec6-85e3-06fb0d9afe79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## K-fold cross-validation for SVM\n",
    "############################\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "# create LDA model\n",
    "res_svc = SVC(kernel=\"linear\", C=1000).fit(X, y)\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_results = cross_val_score(res_svc, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Mean Accuracy:\", cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea8ec3-6fde-4712-8df6-8d0eabdcca97",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LDA\n",
    "\n",
    "Linear Discriminant Analysis (LDA) is used for dimensionality reduction and classification. The goal of LDA is to find the linear combinations of features that separate different classes in the data the best. LDA makes the assumptions that all $f_k(x)$ are normal distributions and all $f_k(x)$ distributions have the same constant variance: $\\sigma^2$. If these assumptions are not met, the performance of LDA may be suboptimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801cf59-a4d4-4b7a-a0de-d013e3fd43de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# LDA\n",
    "###########################################\n",
    "\n",
    "param_grid = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen'], \n",
    "    'store_covariance': [True, False],\n",
    "    'tol': [0.0001, 0.001,0.01, 0.1],\n",
    "    'shrinkage' : [None, 'auto']\n",
    "        }\n",
    "\n",
    "grid_search = GridSearchCV(LinearDiscriminantAnalysis(), \n",
    "                           param_grid=param_grid) \n",
    "grid_search.fit(X, y) \n",
    "print(grid_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49b1cc-3fd9-4ff5-9f61-35dd7c1404fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### KFold Cross Validation for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7af8f7-8b89-47b8-9f93-5223680b0493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable\n",
    "\n",
    "############################\n",
    "## K-fold cross-validation\n",
    "############################\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "# create LDA model\n",
    "LDA = LinearDiscriminantAnalysis(shrinkage='auto', solver='lsqr',\n",
    "                           store_covariance=True)\n",
    "# k-fold cross-validation\n",
    "cv_results = cross_val_score(LDA, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Accuracy Scores:\", cv_results.mean(), cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d1289-dfe0-4bd3-8f8a-c1f221f12d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "175d44cd-5b92-450e-ad26-81bb40b37c49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### QDA\n",
    "\n",
    "Quadratic Discriminant Analysis (QDA) is another aproach that can be taken similar to LDA. The only change between LDA and QDA is the assumption of the variance/covariance. Instead of only one common variance/covariance being assumed, each $k$ class has their own, personal variance/covariance. Doing so, this allows more parameters to be fit by the data. This appraoch gave us a lackluster result with an accuracy far below the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face85f1-fc07-4ade-9ea8-1bfc55e330b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## Feature Selection\n",
    "############################\n",
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd59837-7e83-413b-8c68-6510d7243427",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# QDA\n",
    "###########################################\n",
    "\n",
    "param_grid = {\n",
    "    'store_covariance': [True, False],\n",
    "    'tol': [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "        }\n",
    "\n",
    "grid_search = GridSearchCV(QuadraticDiscriminantAnalysis(), \n",
    "                           param_grid=param_grid) \n",
    "grid_search.fit(X, y) \n",
    "print(grid_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe08b66-de4f-45e6-84f4-e0cb0826324b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### KFold Cross Validation for QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56bba1-52ef-4d04-9487-a968057acbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable\n",
    "\n",
    "############################\n",
    "## K-fold cross-validation - Best\n",
    "############################\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "# create LDA model\n",
    "QDA = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_results = cross_val_score(QDA, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Accuracy Scores:\", cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d13118f-76e2-4c02-817b-a0d909efdd90",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes is another approach similar to above, however, with this method, we assume that all the $p$ predictors have their own personal unique distributions and are completely independent of one another. This method is very computationally efficient with large datasets since it requires a small amount of training data in order to estimate parameters. The \"naive\" assumption can also make Naive Bayes resilient to unimportant features which makes it less sensitive to noisy data. This approach, similarly to QDA, gave us very subpar results in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03091e-6e92-421f-a42b-5a867b5c1c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# NB\n",
    "###########################################\n",
    "\n",
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(-4,4,200)\n",
    "        }\n",
    "\n",
    "grid_search = GridSearchCV(GaussianNB(), \n",
    "                           param_grid=param_grid) \n",
    "grid_search.fit(X, y) \n",
    "print(grid_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144448b9-9b84-4c62-99b4-0376eba34961",
   "metadata": {
    "tags": []
   },
   "source": [
    "### KFold Cross Validation for NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109d4f6-510a-4e77-b6c3-eb7562ffd55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## Feature Selection\n",
    "############################\n",
    "\n",
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable\n",
    "folds = 10 # folds    \n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "# Naive Bayes model\n",
    "naive_bayes = GaussianNB(var_smoothing=0.0001)\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_results = cross_val_score(naive_bayes, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# results\n",
    "print(\"Accuracy Scores:\", cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40547b2a-1a61-4d4e-b572-c0da5fcd0b97",
   "metadata": {},
   "source": [
    "### (Multiple) Logistic Regression\n",
    "\n",
    "Logistic regression is a machine learning model that utilizes the use Log-Odds transformation. The log-odds is defined as the $\\log(p/(1-p))$ and has the range $(-\\infty, \\infty)$. Multiple logistic regression is the modeling of the relationship between 2+ independent features and a response variable. In this model, the response variable represents the probability of an observation belonging to a particular class.\n",
    "\n",
    "One of the pros of logistic regression is it provide insights into the strength and direction of the relationship between each feature and the log-odds of the outcome. Logistic regression tends to perform well in problems where the underlying relationships are approximately linear, but if the true relationship is highly non-linear, the model may not capture it accurately.\n",
    "\n",
    "The hyperparamters we are able to tune in the logistic regression model are the amount of regularization (C), and the optimization algorithm (solver). The regularization parameter, C, is a positive value that controls the inverse of the regularization strength. Smaller values of C exhibit stronger regularization and vice versa. It's typically chosen from a logarithmic scale (logspace) to explore a wide range of values.The solver parameter helps to specify which optimization algorithm is used for fitting the model. Different algorithms have different capabilities and are catered to different types of problems.\n",
    "\n",
    "When comparing this with our Random Forest, we see that the spread of their accuracy scores is fairly similar. The difference, however, lies within the mean accuracy scores of the two. A random forest provides a slightly higher mean accuracy score making it a better choice for a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0dbcf-0bfd-46ff-9d3a-791fe0925361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## Predictors/Response\n",
    "############################\n",
    "X = dfc.iloc[:,0:12] # Features\n",
    "y = dfc.y # Target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb4aa5-1f0c-4202-9375-e0aceb055277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "## Parameter Tuning for LogReg\n",
    "#######################################\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'C':  np.logspace(-4, 4, 200),\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), \n",
    "                           param_grid=param_grid) \n",
    "grid_search.fit(X, y) \n",
    "print(grid_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fbc0a-7507-47d8-a4c0-ec4cfb2e1851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## K-fold cross-validation for Log Reg\n",
    "############################\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "# create LDA model\n",
    "lr = LogisticRegression(C=0.0001, solver='liblinear')\n",
    "\n",
    "# k-fold cross-validation\n",
    "cv_results = cross_val_score(lr, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Mean Accuracy:\", cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74af9b-2877-416c-b761-2462bbc41f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Comparing Logistic Regression and Random Forest\n",
    "###############################################################################\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "mod1 = RandomForestClassifier(n_estimators=150, max_depth=3, max_leaf_nodes=6, random_state=1)\n",
    "mod2 = LogisticRegression(C=0.0001, solver='liblinear', random_state=1)\n",
    "\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "models = [mod1, mod2]\n",
    "\n",
    "for model in models:\n",
    "    cv_results_model = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    cv_results.append(cv_results_model)\n",
    "\n",
    "plt.boxplot(cv_results)\n",
    "plt.title(\"Classification Results\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks([1, 2], [\"Random Forest\", \"LR\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d384fe1-9663-40d2-9c95-6821a15595b4",
   "metadata": {},
   "source": [
    "By constructing another boxplot of every model used so far, we are able to see the spread of accuracy exhibited by each model relative to one another. This shows that our of the five models so far, random forest gives us the best mean accuracy by far. This is followed by Logistic Regression and LDA which both produced very similar levels of accuracy. Coming in last were Naive Bayes and QDA which both gave mean accuracy scores well under the other three models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20cc07-ebc0-456e-bb60-832c00313b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###############################################################################\n",
    "# Comparing All Models - Basic\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "mod1 = RandomForestClassifier()\n",
    "mod2 = LogisticRegression()\n",
    "mod3 = LinearDiscriminantAnalysis()\n",
    "mod4 = QuadraticDiscriminantAnalysis()\n",
    "mod5 = GaussianNB()\n",
    "\n",
    "\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "models = [mod1, mod2, mod3, mod4, mod5]\n",
    "\n",
    "for model in models:\n",
    "    cv_results_model = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    cv_results.append(cv_results_model)\n",
    "\n",
    "plt.boxplot(cv_results)\n",
    "plt.title(\"Classification Results\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks([1, 2, 3, 4, 5], [\"Random Forest\", \"LR\", \"LDA\", \"QDA\", \"NB\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5741fa9-47fe-4c37-a967-8342a433aad5",
   "metadata": {},
   "source": [
    "To further ensure we have selected the best performing model, we construct a final boxplot containing the accuracy score spread for each model while optimized. This ranks the models in the same exact order as it previously did reassuring that this is the correct order of the models. \n",
    "\n",
    "Because of these performances, we have decided to go with a random forest for our final model. This random forest will have 150 estimators, a max depth of 3, and a max leaf nodes of 6. With these hyperparameters set to these values, it allows our random forest model to perform optimally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f1222-920e-4dbc-af73-773b7dd1b1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###############################################################################\n",
    "# Comparing All Models - Optimized\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "mod1 = RandomForestClassifier(n_estimators=150, max_depth=3, max_leaf_nodes=6, random_state=1)\n",
    "mod2 = LogisticRegression(C=0.01, solver='liblinear', random_state=1)\n",
    "mod3 = LinearDiscriminantAnalysis(shrinkage='auto', solver='lsqr',\n",
    "                           store_covariance=True)\n",
    "mod4 = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "mod5 = GaussianNB(var_smoothing=0.0001)\n",
    "\n",
    "\n",
    "folds = 10  # num of folds\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "models = [mod1, mod2, mod3, mod4, mod5]\n",
    "\n",
    "for model in models:\n",
    "    cv_results_model = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    cv_results.append(cv_results_model)\n",
    "\n",
    "plt.boxplot(cv_results)\n",
    "plt.title(\"Classification Results\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks([1, 2, 3, 4, 5], [\"Random Forest\", \"LR\", \"LDA\", \"QDA\", \"NB\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eec5ac-a589-408a-b1b3-1bb61579f1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
